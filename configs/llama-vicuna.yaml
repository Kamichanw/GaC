# This is a configuration file assuming the availability of an 80GiB GPU (A100 or H100).
# If you want to perform ensemble at each generation step, please ensure that the 'priority' for all models is set to 'supportive'. The threshold will be ignored.
NORM_TYPE_API_SERVER: 'average' # 'average' or 'score', 'score' means each model's output vector in the GaC ensemble is weighted by its score divided by the total score.
THRESHOLD_API_SERVER: 1.0
CONFIG_API_SERVER:
  - weight: '/data1/fjl/fjl_copy/pretrained_models/llama-2-7b' # or 'upstage/SOLAR-10.7B-Instruct-v1.0'
    max_memory:
      0: '24GiB'
      1: '24GiB'
    num_gpus: 0.5
    name: 'llama-2-7b'
    score: 100
    priority: 'supportive'
    quantization: 'none' # 'none'/'8bit'/'4bit'
  
  - weight: '/data1/fjl/fjl_copy/pretrained_models/vicuna-7b-v1.5' # or 'openchat/openchat-3.5-0106'
    max_memory:
      0: '24GiB'
      1: '24GiB'
    num_gpus: 0.5
    name: 'llama-2-7b-68m'
    score: 100
    priority: 'supportive'
    quantization: 'none' # 'none'/'8bit'/'4bit'
